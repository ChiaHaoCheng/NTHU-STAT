---
title: "HW 7-Linear model"
author: 
- 'ID : 111024517'
- 'Name : 鄭家豪'
date: due on 01/05
geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
header-includes:
  - \usepackage{leading}
  - \leading{18pt}
  - \usepackage{xeCJK}
  - \setCJKmainfont{標楷體}
  - \setCJKmonofont{標楷體}
output:
  pdf_document:
    latex_engine: xelatex
classoption: a4paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1  

## Read Data
```{r}
data <- read.table("http://www.stat.nthu.edu.tw/~swcheng/Teaching/stat5410/data/aatemp.txt",
                   header = T)
head(data)
```

## i  
 
 建構 simple linear model:  
 
$$
\text{temp}_i = \beta_0 + \beta_1 \text{year}_i + \epsilon_i \ \ ,\text{where} \ \epsilon_i \sim N(0,\sigma^2)
$$
 
```{r}
fit_1 <- lm(temp~.,data=data)
summary(fit_1)
```
 
 我們得到 fit model:  

$$
\text{temp}_i = 24.005510 + 0.012237 \text{year}_i
$$
 接著加入 year 的二次項至 model:
```{r}
fit_2 <- lm(temp ~ poly(year,2,raw=T),
            data = data)
summary(fit_2)
```
 
 這會使得 year 一次項 和 二次項 的係數皆不顯著，因此不考慮加入二次項。  
 接著觀察一次項模型的 $\beta_1$  95% 信賴區間
 
```{r}
confint(fit_1,level = 0.95)
```
 
 其 95% 信賴區間: (0.004771599,0.01970293)  
 由於不包含 0 ，因此在顯著水準 0.05下，有 linear trend 。

```{r echo=FALSE}
plot(y=data$temp,x=data$year,xlab = "year",ylab="temp")
abline(fit_1,col="red")
legend(1850,52,
       legend=c("Fitting line"),
       lty=1,cex=0.7,col = "red")
```

## ii

 使用 package: "nlme" ，進行 fit the model with correlated error following an AR(1) structure :  
```{r}
library(nlme)
fit_ar <- gls(temp~year,correlation = corAR1(form = ~year),
              data=data)
intervals(fit_ar,level = 0.95)
```
 
 我們得到 the estimated correlation $\rho= 0.2303887$ ，且在顯著水準 0.05 下，拒絕 $\rho= 0$ 的假設。  
 接著，在此模型下， year 的 95% 信賴區間不包含 0 ，因此不改變 trend 的看法。  

## iii

 建立 year 十次多項式的模型:  
 
```{r}
fit_10 <- lm(temp~poly(year,10),data = data)
summary(fit_10)
```

 可以觀察到，第六項之後的變數皆不顯著，由於 orthogonality ，因此直接將第六項至第十項直接移除，保留前五項的變數再建構模型。  
 
```{r}
fit_5 <- lm(temp~poly(year,5),data=data)
summary(fit_5)
```
 
```{r echo=FALSE}
plot(x=data$year,y=data$temp,xlab = "year",ylab = "temp")
lines(fitted(fit_5) ~ data$year ,col="red")
legend(1850,52,
       legend=c("Fitting line of 5 poly"),
       lty=1,cex=0.7,col = "red")
```
 
 接著預測 2020 年的 temperature:  
 
```{r}
predict(fit_5,data.frame(year=2020),
                         se=TRUE,
                         interval = "prediction")
```
 我們得到 預測值: 60.07774 ，和 95% 預測區間: (49.84092,70.31456) 。  
 
## iv  
 
 Define the base function:  

$$
d(\text{year}) = 
\begin{cases}
1 & \text{ if } \text{year}>1930\\
0 & \text{ if } \text{otherwise}
\end{cases}
$$

### Broken line regression(No continuity)

 The Model:  
 
$$
\text{temp}_i = \beta_0 + \beta_1 d(\text{year}_i) + \beta_2 \text{year}_i+\beta_3(\text{year}_i-1930)d(\text{year}_i) +\epsilon_i \ \ ,\text{where} \ \epsilon_i \sim N(0,\sigma^2)
$$
 
```{r}
d <- function(x){ifelse(x>1930,1,0)}
model_broken1 <- lm(temp~ d(year) + year + I((year - 1930) * d(year)),
                   data=data)
summary(model_broken1)
```
 
 得到模型:  

$$
\text{temp}_i = 54.452092   + 1.853081 d(\text{year}_i) -0.003915 \text{year}_i-0.008603(\text{year}_i-1930)d(\text{year}_i)
$$
 由以上模型，繪製出其 fitting line :  

```{r echo=FALSE}
fit_before <- lm(temp ~ year , data=data,subset = (year<=1930))
fit_after <- lm(temp ~ year, data=data,subset = (year > 1930))
plot(data$year, data$temp, 
     xlab="year", ylab="temp")
abline(v=1930, lty=5)
segments(1854, fit_before$coef[1]+fit_before$coef[2]*1854, 
         1930, fit_before$coef[1]+fit_before$coef[2]*1930,
         col = "blue")
segments(2000, fit_after$coef[1]+fit_after$coef[2]*2000, 
         1930, fit_after$coef[1]+fit_after$coef[2]*1930,
         col="red")
legend(1850,52,
       legend=c("Fitting line:Before 1930",
                "Fitting line:After 1930"),
       lty=c(1,1),cex=0.7,col = c("blue","red"))
```
 
 觀察這張圖來判斷 Claim是否合理，由圖和模型斜率係數的顯著性來看，1930年之後的斜率變化似乎沒有很顯著，因此這個 Claim 似乎不正確。  

### Broken line regression(continuity)
 
 為了在 year = 1930 時連續，模型修改為:  
 
$$
\text{temp}_i = \beta_0 + \beta_1 \text{year}_i + \beta_2 (\text{year}_i-1930)d(\text{year}_i) +\epsilon_i \ \ ,\text{where} \ \epsilon_i \sim N(0,\sigma^2)
$$
```{r}
model_broken2 <- lm(temp~ year + I((year - 1930) * d(year)),
                   data=data)
summary(model_broken2)
```
 
```{r echo=FALSE}
py <- model_broken2$coef[1] + model_broken2$coef[2]*data$year + model_broken2$coef[3] * ((data$year-1930)*d(data$year))
plot(data$year, data$temp, 
     xlab="year", ylab="temp")
abline(v=1930, lty=5)
lines(data$year,py,lty=1,col ="red")
legend(1850,52,
       legend=c("Fitting line"),
       lty=c(1),cex=0.7,col = c("red"))
```
 
 由這張圖和模型 summary 來看，由於 (year - 1930) * d(year) 項的係數不顯著，因此不太能接受這個 Claim 是正確的。  

## v  
 
 根據 LNp.8-8 的規則，選取 6+4 個 knots:  
 
```{r}
knots <- c(1854,1854,1854,1854,1921,1962,2000,2000,2000,2000)
```

 接著使用 package: "splines"，進行 cubic spline fit:  
 
```{r}
library(splines)
base <- splineDesign(knots,data$year)
model_cubic <- lm(temp~base,data=data)
summary(model_cubic)
```
 
 $R^2 = 0.2044$，比較 i 的模型 $R^2=0.08536$，B-spline 會比 simple linear model 還好些。  
```{r echo=FALSE}
matplot(data$year,base,type="l",main="B-spline basis functions")
```
 
* Plot the fit in comparison to the previous fits:  
 
```{r echo=FALSE}
plot(temp~year, data=data)
abline(fit_1$coef,col = "black")
lines(fitted(fit_5) ~ data$year ,col="blue")
lines(data$year,py,lty=1,col ="green")
lines(data$year,predict(model_cubic),lty=1,col="red")
legend(1850,52,
       legend=c("simple","5-poly","broken line","B-spline"),
       lty=1,cex=0.7,
       col = c("black","blue","green","red"))
```
 
 將前面所得到的fitting line(除了 10-poly. 和 unconutious broken regression)，都繪至在同一張圖上，可發現 5-polynomial model 和 Cubic B-spline model 的表現會其他模型還要好，其中這意味著溫度會隨著時間變化而有所變化。  
 
# Q2

## Read data
 
 在進行讀取前，需要對資料做調整方便讀取:  
 
```{r echo=FALSE}
file <- "123123.png"
knitr::include_graphics(file)
```

```{r}
data <- read.table("E1.20.txt",skip =3)
colnames(data) <- c("state","PQLI","Comb.IMR",
                     "Rur.M.IMR","Rur.F.IMR",
                     "Urb.M.IMR","Urb.F.IMR")

head(data)
```
 
 我們的目的是想研究 IMR 與性別和區域的關係，由於原始資料是將性別區域IMR合併在一起，分成 "Rur.M.IMR" 、 "Rur.F.IMR" 、 "Urb.M.IMR" 、 "Urb.F.IMR" ，為了便於分析，定義兩個 dummy variable 來拆成三個 column ，再加入對應的 PQLI，整合成新的資料(名稱為"data_combin") :  
$$
d_1(\text{gender}) = \begin{cases}
1 & \text{ if } \text{gender is Male} \\
0 & \text{ if }  \text{gender is Female}
\end{cases}
\ \ , \ 
d_2(\text{Area}) = \begin{cases}
1 & \text{ if } \text{area is urban} \\
0 & \text{ if }  \text{area is rural}
\end{cases}
$$
```{r}
data_combin <- data.frame("MIR" = c(data$Rur.M.IMR,data$Rur.F.IMR,data$Urb.M.IMR,data$Urb.F.IMR),
                     "Gender" = c(rep("Male",13),rep("Female",13),rep("Male",13),rep("Female",13)),
                     "Area" = c(rep("Rural",26),rep("urban",26)),
                     "PQLI"= rep(data$PQLI,4))
dim(data_combin)
names(data_combin)
```
 
 利用這資料來做分析:  
 
## ANCOVA  

```{r}
fit1 <- lm(MIR ~  Gender + Area ,data = data_combin)
fit2 <- lm(MIR ~  Gender + Area + PQLI,data = data_combin)
anova(fit1,fit2)
``` 
 
 Obviously,as p-value is small than 0.05 ,the quantiative predictor PQLI is covariate.  
 
## Fit model  

 配適一個 MIR ~ Gender * Area * PQLI 的模型:  
 
```{r}
fit <- lm(MIR ~ Gender*Area*PQLI,data=data_combin)
summary(fit)
```
 
 此時應該要考慮 PQLI 和 Gender 或 Area 之間是否有交互作用效應，使用 anova() 指令觀察:  
 
```{r}
anova(fit)
```
 
 由上面結果，只有 Area 和 PQLI 的交互作用項顯著，另外 Gender 項不顯著，於是移除 Gender 項和加入 Area:PQLI 交互作用項然後重配模型:  
 
```{r}
fit_new <- lm(MIR ~ Area + PQLI + Area:PQLI,data=data_combin)
summary(fit_new)
```
 
 我們得到模型:  
 
$$
\text{MIR} = 180.2417 -72.6564 \times d_2(\text{area}) -1.4702\times \text{PQLI} + 0.5928 \times (d_2(\text{area}) \times \text{PQLI})
$$
 
 我們可以看到，每個項的係數皆是顯著，there exist rural-urban difference in mortality after adjusting for the covariate,PQLI.  

# Q3  

## Read data and fit simple linear model  

```{r}
data <- read.table("http://www.stat.nthu.edu.tw/~swcheng/Teaching/stat5410/data/cornnit.txt",
                   header = T)
head(data)
```

```{r}
fit <- lm(yield~.,data=data)
summary(fit)
```
 
 We have model:  
$$
\text{yield}_i = 107.43864 + 0.17730 (\text{nitrogen}_i)
$$

## Testing for Lack of fit 
 
 用 anova() 指令進行 Testing for Lack of fit:  
 
```{r}
fit_sature <- lm(yield ~ factor(nitrogen),data=data)
anova(fit,fit_sature)
```
 
 由於 p-value <0.05 ，有足夠證據表示這個模型配得不好。  
 
## Box-Cox method  
 
 這裡檢查是否適合使用 Box-Cox method for response，Using package : "MASS"。  
 
```{r}
library(MASS)
boxcox(fit,plotit = T)
```
 
 由於 lambda 值似乎超過 1 之後，log-likelihood 還在增加，我們試著把 lambda 的範圍往後拉一點:  
 
```{r}
boxcox(fit,plotit = T,lambda = c(0,4,1/100))
```
 
 可以發現當 $\lambda \in (2,3)$ 時，其 log-likelihood 會達至最大。  
 我們試著對 response 做 $(y^{3}-1)/3$ 的 transformation (比較有解釋性且3比較靠近最大 log-likelihood 的 $\lambda$)，然後 fit model:  
 
```{r}
g1 <- lm(I(yield^3) ~ nitrogen,data=data)
summary(g1)
```
```{r}
821600^(1/3)
```

 其 $\hat \sigma = 821600$，由於單位是 response 單位的3次方，算回去原本的單位後，得到 93.65985 > 20.53($\hat \sigma$ from the model without transformation)。然後 $R^2=0.4084$ 相較於 沒轉換後的 $R^2=0.3962$ ，差不了多少，因此對 response 做 Box-Cox transformation 對於模型沒有改善。  
 我們來檢驗是否要對 predictor 做 transformation:  
 先觀察 nitrogen-residual 之間的關係:
 
```{r echo=FALSE}
plot(x = data$nitrogen,
     y = fit$residuals,
     xlab="nitrogen",ylab="Residual")
```
 
 由以上的圖，可以觀察出 nitrogen 和 residual 似乎有"凹口向下"的曲線關係(second derivative is small than 0)，因此試著對 nitrogen 做 Box-Cox transformation($x^{\lambda}$,$\lambda \in (0,1)$)。  
 定義:  
 
$$
xlog(x) = \begin{cases}
xlog(x) & \text{ if } \text{x}>0\\
0 & \text{ if }  \text{otherwise}
\end{cases}
$$
 
 這樣定義的目的是為了使 nitrogen = 0 時有意義($0^{\lambda}=0$)  
 接著建構 model:  
$$
\text{yield}_i = \beta_0 + \beta_1(\text{nitrogen}+(\lambda-1)\text{nitrogen}\times \log(\text{nitrogen})) + \epsilon_i \ \ ,\text{where} \ \epsilon_i \sim N(0,\sigma^2)
$$
```{r}
f = function(x){
  c=c()
  for (i in 1:length(x)){
    if(x[i] == 0){c[i]=0}
    else{c[i]=x[i]*log(x[i])}
  }
  return(c)
}
g2 <- lm(yield ~ nitrogen + f(data$nitrogen),data=data)
summary(g2)
```

 由於 $x_i log(x_i)$ 項的係數顯著不為 0 ，因此對 nitrogen 做轉換($\lambda = \dfrac{-0.30757}{1.90973}+1=0.8389458$):  
 轉換完後重新 fit:  
```{r}
fit_trans <- lm(yield ~ I(nitrogen^0.8389458),data = data)
summary(fit_trans)
```
 
 轉換完後，$R^2 = 0.4571$ and $\hat \sigma = 19.47$ ，與沒轉換的模型做比較，其 $R^2 = 0.3962$ and $\hat \sigma = 20.53$ ，模型有得到改善。不過缺點是犧牲兩者變數之間的解釋性。  
 
```{r}
anova(fit_trans,fit_sature)
```
 
 雖然還是有 lack of fit ，但p-value 相比沒轉換前，有增加很多，這意味著對 predictor 轉換後確實有得到些許改善。  
 
 我們最後用 shapiro.test() 來檢定 對 predictor 轉換前和轉換後的模型哪個比較符合 normality:  
```{r echo=FALSE}
shapiro.test(rstandard(fit))
shapiro.test(rstandard(fit_trans))
```
 
 若顯著水準 = 0.05 ，那這兩個模型都符合 normality，但因為轉換後的模型， p-value 較轉換前的模型還要大，所以轉換後比轉換前更加符合 normality。
 

